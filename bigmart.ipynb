{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Business Problem (Regression Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the business problem that you are trying to solve. <br>What is the business context and what are the constraints you might face.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigMart Sales Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a predictive model and find out the sales of each product at a particular store.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The data scientists at BigMart have collected 2013 sales data for numerous products across many stores in different cities. Also, certain attributes of each product and store have been defined.<br>\n",
    "Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Item_Identifier**: Unique Number assigned to each Item.\n",
    "- **Item_Weight**: Item Weight in g.\n",
    "- **Item_Fat_Content**: Item Fat Content.\n",
    "- **Item_Visibility**: Placement value of each item: 0 - Far & Behind 1 - Near & Front.\n",
    "- **Item_Type**: Type of item utility.\n",
    "- **Item_MRP**: Price of the Item.\n",
    "- **Outlet_Identifier**: Unique Outlet Name.\n",
    "- **Outlet_Establishment_Year**: Year of Outlet Establishment.\n",
    "- **Outlet_Size**: Size of the Outlet.\n",
    "- **Outlet_Location_Type**: Tier of Outlet Location.\n",
    "- **Outlet_Type**: Type of Outlet.\n",
    "- **Item_Outlet_Sales**: Target variable; Total Sales of the Outlet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning, transforming, and preparing the data for analysis. <br>This step includes handling missing values, dealing with outliers, and ensuring data quality.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.impute import KNNImputer\n",
    "from colorama import Fore, Style\n",
    "import pandas_profiling\n",
    "from pandas_profiling import ProfileReport\n",
    "import scipy.stats as stats\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score,mean_squared_error, mean_absolute_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA15</td>\n",
       "      <td>9.30</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>249.81</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>3735.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRC01</td>\n",
       "      <td>5.92</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Soft Drinks</td>\n",
       "      <td>48.27</td>\n",
       "      <td>OUT018</td>\n",
       "      <td>2009</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type2</td>\n",
       "      <td>443.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FDN15</td>\n",
       "      <td>17.50</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Meat</td>\n",
       "      <td>141.62</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>2097.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FDX07</td>\n",
       "      <td>19.20</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Fruits and Vegetables</td>\n",
       "      <td>182.09</td>\n",
       "      <td>OUT010</td>\n",
       "      <td>1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Grocery Store</td>\n",
       "      <td>732.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCD19</td>\n",
       "      <td>8.93</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Household</td>\n",
       "      <td>53.86</td>\n",
       "      <td>OUT013</td>\n",
       "      <td>1987</td>\n",
       "      <td>High</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>994.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Item_Identifier  Item_Weight Item_Fat_Content  Item_Visibility  \\\n",
       "0           FDA15         9.30          Low Fat             0.02   \n",
       "1           DRC01         5.92          Regular             0.02   \n",
       "2           FDN15        17.50          Low Fat             0.02   \n",
       "3           FDX07        19.20          Regular             0.00   \n",
       "4           NCD19         8.93          Low Fat             0.00   \n",
       "\n",
       "               Item_Type  Item_MRP Outlet_Identifier  \\\n",
       "0                  Dairy    249.81            OUT049   \n",
       "1            Soft Drinks     48.27            OUT018   \n",
       "2                   Meat    141.62            OUT049   \n",
       "3  Fruits and Vegetables    182.09            OUT010   \n",
       "4              Household     53.86            OUT013   \n",
       "\n",
       "   Outlet_Establishment_Year Outlet_Size Outlet_Location_Type  \\\n",
       "0                       1999      Medium               Tier 1   \n",
       "1                       2009      Medium               Tier 3   \n",
       "2                       1999      Medium               Tier 1   \n",
       "3                       1998         NaN               Tier 3   \n",
       "4                       1987        High               Tier 3   \n",
       "\n",
       "         Outlet_Type  Item_Outlet_Sales  \n",
       "0  Supermarket Type1            3735.14  \n",
       "1  Supermarket Type2             443.42  \n",
       "2  Supermarket Type1            2097.27  \n",
       "3      Grocery Store             732.38  \n",
       "4  Supermarket Type1             994.71  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('bigmart.csv')\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Item_Identifier.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Item_Identifier.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Item_Identifier=='FDW13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T.style.background_gradient(cmap='GnBu').\\\n",
    "                    bar(subset=[\"std\"], color='#BB0000').bar(subset=[\"mean\",], color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.astype('object').describe().T.style.background_gradient(cmap='GnBu').\\\n",
    "                                    bar(subset=[\"count\"], color='#BB0000').bar(subset=[\"unique\",], color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicated data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodub=df.drop_duplicates() \n",
    "print(df.shape,df_nodub.shape,'\\n Number of duplicate data : ',df.shape[0]-df_nodub.shape[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is no duplicate data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), cmap='viridis', cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore, Style\n",
    "def pcnt_miss_values(df):\n",
    "    col_names = df.columns\n",
    "    print('\\nThe percentage of miss values for those columns having missing value  \\n')\n",
    "    for col in col_names:\n",
    "        n_value = df[col].isnull().sum()\n",
    "        if n_value>0 :\n",
    "            null_pcnt = round((n_value / df.shape[0])*100 , 2) \n",
    "            print(Fore.RED + Style.BRIGHT+'----> The percentage of null values for column {0} is {1} % \\n '.format(df[col].name , null_pcnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcnt_miss_values(df.drop(['Item_Outlet_Sales'],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.loc[df.Item_Weight.isnull(),'Item_Weight'] = df.Item_Weight.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### impute Outlet_Size with mode                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.loc[df.Outlet_Size.isnull() , 'Outlet_Size']  =df.Outlet_Size.mode()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.Outlet_Size.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute item_weight using average of item_identifier "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "item_avg_weight = df.pivot_table(values='Item_Weight',index='Item_Identifier')\n",
    "miss_bool = df.Item_Weight.isnull()\n",
    "miss_bool.sum()\n",
    "df.loc[miss_bool, 'Item_Weight'] \n",
    "df.loc[miss_bool, 'Item_Weight']  =df.loc[miss_bool, 'Item_Identifier'].apply(lambda x:item_avg_weight.at[x, 'Item_Weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using KNN Imputer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seprate numerical features first\n",
    "import numpy as np\n",
    "num_cols = df.select_dtypes(include=np.number).columns\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "knn  = KNNImputer(n_neighbors=5)\n",
    "df_filled = pd.DataFrame(knn.fit_transform(df[num_cols]),columns=num_cols)\n",
    "df_filled = pd.concat([df.drop(columns=num_cols),df_filled],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filled=df.copy()\n",
    "# df_filled.loc[df_filled['Item_Weight'].isnull(),'Item_Weight']=df_filled.Item_Weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled[df_filled.Item_Identifier=='FDW13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_filled.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardise Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Item_Fat_Content'].isin(['LF', 'low fat']), 'Item_Fat_Content']='Low Fat'\n",
    "df.loc[df['Item_Fat_Content']=='reg','Item_Fat_Content']='Regular'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we decided to categirize these types into 7 major category : \n",
    "\n",
    "\n",
    "    \n",
    "    * Proteins and Main Dishes:\n",
    "        Meat\n",
    "        Seafood\n",
    "   \n",
    "    * Carbohydrates and Staples:\n",
    "        Breads\n",
    "        Starchy Foods\n",
    "   \n",
    "    * Dairy and Alternatives:\n",
    "        Dairy\n",
    "   \n",
    "    * Fruits and Vegetables:\n",
    "        Fruits and Vegetables\n",
    "\n",
    "    * Processed and Convenience Foods:\n",
    "        Snack Foods\n",
    "        Frozen Foods\n",
    "        Canned\n",
    "        Baking Goods\n",
    "        Breakfast\n",
    "    \n",
    "    *Beverages:\n",
    "        Soft Drinks\n",
    "        Hard Drinks\n",
    "\n",
    "    *Household and Others:\n",
    "        Household\n",
    "        Health and Hygiene\n",
    "        Others\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beverages=['Soft Drinks','Hard Drinks']\n",
    "Household_and_Others=['Household','Health and Hygiene','Others']\n",
    "Proteins = ['Seafood','Meat']\n",
    "Processed = ['Snack Foods','Frozen Foods','Canned','Baking Goods','Breakfast']\n",
    "Carbohydrates = ['Starchy Foods','Breads']\n",
    "\n",
    "# df.loc[(~ df['Item_Type'].isin(Beverages)) & (~ df['Item_Type'].isin(Household_and_Others)),'Item_Type'] = 'Food'\n",
    "\n",
    "df.loc[df['Item_Type'].isin(Beverages),'Item_Type'] = 'Beverages'\n",
    "df.loc[df['Item_Type'].isin(Household_and_Others),'Item_Type'] = 'Household_and_Others'\n",
    "df.loc[df['Item_Type'].isin(Proteins),'Item_Type'] = 'Proteins'\n",
    "df.loc[df['Item_Type'].isin(Processed),'Item_Type'] = 'Processed'\n",
    "df.loc[df['Item_Type'].isin(Carbohydrates),'Item_Type'] = 'Carbohydrates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Outlet_Establishment_Year=df.Outlet_Establishment_Year.astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Using randomforrest classifier to impute outlet_size , RandomForestClassifier is designed for classification tasks and works with numeric features , so i have to first encode other features to perform randomforrest classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def num_boxplot(df) :\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != 'object':\n",
    "            plt.figure(figsize=(15,8))\n",
    "            sns.boxplot(x=col,data=df)\n",
    "            plt.title('Box plot for '+col+'\\n')\n",
    "            print(Fore.RED , Style.BRIGHT,'\\033[1m ',100*'=')\n",
    "            plt.show()\n",
    "            \n",
    "num_boxplot(df.drop('Item_Outlet_Sales',axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### as we can see in boxplot , it seems that only Item_Visibility  has some oulliers  , now i test it with IQR and find the extreme outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(df,col) :\n",
    "    if df[col].dtype != 'object':\n",
    "        q1=df[col].quantile(.25)\n",
    "        q3=df[col].quantile(.75)\n",
    "        iqr = q3-q1 \n",
    "        lower_b = q1 - 1.5 * iqr\n",
    "        ex_lower_b = q1-3 * iqr\n",
    "        upper_b = q3+1.5 * iqr\n",
    "        ex_upper_b = q3 + 3 * iqr\n",
    "        return ex_lower_b , ex_upper_b , upper_b , lower_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_number_of_ext_outliers(df,label):\n",
    "    list_col=[]\n",
    "    df = df.drop(label , axis=1 )\n",
    "    for col in df.columns :\n",
    "        if df[col].dtype != 'object':\n",
    "            ex_lower_b , ex_upper_b,  upper_b , lower_b = iqr(df,col)\n",
    "            num_ex_outliers = df[(df[col]>ex_upper_b) | (df[col]<ex_lower_b)].shape[0]\n",
    "            if num_ex_outliers > 0 :\n",
    "                print('\\n \\033[1m Number of Exterme Outliers for {0}: ( values > {1} or values <{2})\\n'.format(col,ex_upper_b,ex_lower_b))\n",
    "                print(Fore.RED ,'\\033[1m', Style.BRIGHT,(num_ex_outliers))\n",
    "                print(Style.RESET_ALL)\n",
    "                list_col.append(col)\n",
    "\n",
    "    if len(list_col)==0 :\n",
    "        print('There is no any exterme outliers in dataset ')                \n",
    "    else:\n",
    "        return list_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers (df, colname) :\n",
    "    if len(colname) >0 :\n",
    "        for col in colname:\n",
    "            ex_lower_b , ex_upper_b  , upper_b , lower_b= iqr(df,col)\n",
    "            df = df[(df[col]<ex_upper_b) & (df[col]>ex_lower_b)]\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col_outliers = find_number_of_ext_outliers(df,'Item_Outlet_Sales')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### drop outliers\n",
    "the number of extreme values are too less , so we decided to remove those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result= drop_outliers(df,list_col_outliers)\n",
    "df_result.shape[0] - df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check if there is any outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col_outliers = find_number_of_ext_outliers(df_result,'Item_Outlet_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result1= drop_outliers(df_result,list_col_outliers)\n",
    "df_result1.shape[0] - df_result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col_outliers = find_number_of_ext_outliers(df_result1,'Item_Outlet_Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Outliers : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_number_of_pot_outliers(df,label):\n",
    "    list_col=[]\n",
    "    df = df.drop(label , axis=1 )\n",
    "    for col in df.columns :\n",
    "        if df[col].dtype != 'object':\n",
    "            ex_lower_b , ex_upper_b,upper_b,lower_b = iqr(df,col)\n",
    "            num_ex_outliers = df[(df[col]>upper_b) | (df[col]<lower_b)].shape[0]\n",
    "            if num_ex_outliers > 0 :\n",
    "                print('\\n \\033[1m Number of Potenitial Outliers for {0}: ( values > {1} or values <{2})\\n'.format(col,ex_upper_b,ex_lower_b))\n",
    "                print(Fore.RED ,'\\033[1m', Style.BRIGHT,(num_ex_outliers))\n",
    "                print(Style.RESET_ALL)\n",
    "                list_col.append(col)\n",
    "\n",
    "    if len(list_col)==0 :\n",
    "        print('There is no any Potenitial outliers in dataset ')                \n",
    "    else:\n",
    "        return list_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_pot_outliers (df, colname) :\n",
    "    if len(colname) >0 :\n",
    "        for col in colname:\n",
    "            ex_lower_b , ex_upper_b,upper_b,lower_b = iqr(df,col)\n",
    "            df = df[(df[col]<upper_b) & (df[col]>lower_b)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col_outliers = find_number_of_pot_outliers(df_result1,'Item_Outlet_Sales')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result2= drop_pot_outliers(df_result1,list_col_outliers)\n",
    "df_result1.shape[0] - df_result2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col_outliers = find_number_of_pot_outliers(df_result2,'Item_Outlet_Sales')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result1= drop_pot_outliers(df_result2,list_col_outliers)\n",
    "df_result2.shape[0] - df_result1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col_outliers = find_number_of_pot_outliers(df_result1,'Item_Outlet_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result2= drop_pot_outliers(df_result1,list_col_outliers)\n",
    "df_result1.shape[0] - df_result2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col_outliers = find_number_of_pot_outliers(df_result2,'Item_Outlet_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_result2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting familiar with all features and doing some replacemen , extracion , discritizing ,etc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring the data visually and statistically to gain a better understanding of its characteristics and relationships.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Item_Outlet_Sales']\n",
    "colors = ['purple', 'g', 'b', 'c'] \n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "for i in range(len(numeric_columns)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.distplot(df[numeric_columns[i]], bins=20, color=colors[i])\n",
    "    plt.xlabel(numeric_columns[i])\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The distribution of Item_Weight is basically flat. Only when the weight is 13g, the number surges.<br>\n",
    "<br>\n",
    "The shapes of Item_Visibility and Item_Outlet_Sales are basically the same. <br>\n",
    "It can be intuitively seen that the placement of the item directly affects sales. <br>\n",
    "The further away the placement, the lower the sales.<br>\n",
    "<br>\n",
    "Item_MRP fluctuates greatly, proving that the price difference between items is large.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Establishment_Year',\n",
    "                       'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n",
    "colors = ['CornflowerBlue', 'MediumSeaGreen', 'Tomato', 'Plum', 'RoyalBlue', 'LightGreen', 'Coral' ]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i in range(len(categorical_columns)):\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    sns.barplot(x=df[categorical_columns[i]].value_counts(normalize=True).index,\n",
    "                y=df[categorical_columns[i]].value_counts(normalize=True) * 100, color=colors[i])\n",
    "    plt.xlabel(categorical_columns[i])\n",
    "    plt.ylabel('Percentage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_pp = ['Item_Fat_Content', 'Outlet_Location_Type', 'Outlet_Type']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "for i in range(len(categorical_columns_pp)):\n",
    "    value_counts = df[categorical_columns_pp[i]].value_counts()\n",
    "    explode = [0.1 if idx == value_counts.idxmax() else 0 for idx in value_counts.index]\n",
    "    \n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.pie(df[categorical_columns_pp[i]].value_counts().values, explode=explode,\\\n",
    "            labels=df[categorical_columns_pp[i]].value_counts().index, \\\n",
    "            autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "    plt.xlabel(categorical_columns_pp[i])\n",
    "    plt.ylabel('Percentage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(categorical_columns)):\n",
    "    value_counts_percentage = df[categorical_columns[i]].value_counts(normalize=True) * 100\n",
    "    print(f'Percentage distribution for {categorical_columns[i]}:')\n",
    "    print(value_counts_percentage)\n",
    "    print('_' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Item_Fat_Content:<br>\n",
    "The proportion of low fat products is significantly higher than that of regular fat products.<br>\n",
    "<br>\n",
    "For Item_Type:<br>\n",
    "The proportion of Fruits and Vegetables is close to that of Snack Foods, and they are the two categories with the highest proportions. After that, Household and Frozen Foods have the highest proportions.<br>\n",
    "<br>\n",
    "For Outlet_Identifier:<br>\n",
    "The proportions are generally the same, only the proportions of #10 and #19 are slightly lower.<br>\n",
    "<br>\n",
    "For Outlet_Establishment_Year:<br>\n",
    "The proportions are roughly the same every year, but the proportion in 1985 is slightly higher and the proportion in 1998 is slightly lower.<br>\n",
    "<br>\n",
    "For Outlet_Size:<br>\n",
    "Among Outlet_Size, medium has the highest proportion.<br>\n",
    "<br>\n",
    "For Outlet_Location_Type:<br>\n",
    "The higher the Tier, the higher the proportion.<br>\n",
    "<br>\n",
    "For Outlet_Type:<br>\n",
    "The proportion of Supermarket Type 1 is 65%, far exceeding the sum of the other three types of outlets.<br>\n",
    "<br>\n",
    "For Category:<br>\n",
    "Food accounts for 72% of all items and is the main product**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Item_Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Item_Identifier.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Item_Identifier.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Outlet_Identifier.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lets check the correlation with other features in bi-variate analysis then decist about it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item_Fat_Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes['Item_Fat_Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Item_Fat_Content.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['Item_Fat_Content'].isin(['LF', 'low fat']), 'Item_Fat_Content']='Low Fat'\n",
    "# df.loc[df['Item_Fat_Content']=='reg','Item_Fat_Content']='Regular'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Item_Fat_Content.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item_Visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes['Item_Visibility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Item_Visibility.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df , x='Item_Visibility' , kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item_Type                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes['Item_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Item_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beverages=['Soft Drinks','Hard Drinks']\n",
    "# Household_and_Others=['Household','Health and Hygiene','Others']\n",
    "# Proteins = ['Seafood','Meat']\n",
    "# Processed = ['Snack Foods','Frozen Foods','Canned','Baking Goods','Breakfast']\n",
    "# Carbohydrates = ['Starchy Foods','Breads']\n",
    "\n",
    "# # df.loc[(~ df['Item_Type'].isin(Beverages)) & (~ df['Item_Type'].isin(Household_and_Others)),'Item_Type'] = 'Food'\n",
    "\n",
    "# df.loc[df['Item_Type'].isin(Beverages),'Item_Type'] = 'Beverages'\n",
    "# df.loc[df['Item_Type'].isin(Household_and_Others),'Item_Type'] = 'Household_and_Others'\n",
    "# df.loc[df['Item_Type'].isin(Proteins),'Item_Type'] = 'Proteins'\n",
    "# df.loc[df['Item_Type'].isin(Processed),'Item_Type'] = 'Processed'\n",
    "# df.loc[df['Item_Type'].isin(Carbohydrates),'Item_Type'] = 'Carbohydrates'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df.Item_Type.value_counts(normalize=True) * 100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item_MRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes['Item_MRP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df ,x='Item_MRP',kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stat, p = stats.shapiro(df.Item_MRP)\n",
    "\n",
    "if p<0.05 :\n",
    "    print('Data does not look normally distributed (reject H0) - Pvalue is : ',p)\n",
    "else :\n",
    "    print('Data does looks normally distributed (cannot reject H0) - Pvalue is : ',p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlet_Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes['Outlet_Identifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Outlet_Identifier.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlet_Establishment_Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes['Outlet_Establishment_Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Outlet_Establishment_Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.Outlet_Establishment_Year=df.Outlet_Establishment_Year.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes['Outlet_Establishment_Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Outlet_Establishment_Year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlet_Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes['Outlet_Size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Outlet_Size.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlet_Location_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes['Outlet_Location_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Outlet_Location_Type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlet_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes['Outlet_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Outlet_Type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric - Categorical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item_Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.scatterplot(df, x='Item_Weight', y='Item_Outlet_Sales',hue=\"Item_Outlet_Sales\", palette='RdYlBu', s = 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sales in each range of item weight are roughly the same.<br>\n",
    "The relationship between item weight and item sales cannot be seen from this plot.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item_MRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.scatterplot(df, x='Item_MRP', y='Item_Outlet_Sales',hue=\"Item_Outlet_Sales\", palette='RdYlBu', s = 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As the item MRP increases, item sales also increase significantly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item_Visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.scatterplot(df, x='Item_Visibility', y='Item_Outlet_Sales',hue=\"Item_Outlet_Sales\", palette='RdYlBu', s = 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.scatterplot(df, x='Item_Visibility', y='Item_Outlet_Sales',hue=\"Item_Outlet_Sales\", palette='RdYlBu', s = 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most items are placed far away. Therefore, sales are generally higher than those in the front area.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric - Categorical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item_Fat_Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4,4))\n",
    "sns.barplot(df, x='Item_Fat_Content', y='Item_Outlet_Sales', ci=None, palette='RdYlBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.swarmplot(df,x='Item_Fat_Content',y='Item_Outlet_Sales',hue='Item_Outlet_Sales', palette='RdYlBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is not much difference in the item sales between low fat and regular fat.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,4))\n",
    "sns.barplot(df, x='Item_Type', y='Item_Outlet_Sales', ci=None, palette='RdYlBu')\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.swarmplot(df,x='Item_Type',y='Item_Outlet_Sales',hue='Item_Outlet_Sales', palette='RdYlBu')\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is not much difference in sales between item types.<br>\n",
    "The number of breakfast and seafood items is small, but the sales are still at the overall average level.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,4))\n",
    "sns.barplot(df, x='Item_Type', y='Item_MRP', ci=None, palette='RdYlBu')\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The difference in MRP between different item types is very small and basically the same.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlet_Establishment_Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,4))\n",
    "sns.barplot(df, x='Outlet_Establishment_Year', y='Item_Outlet_Sales', ci=None, palette='RdYlBu')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.swarmplot(df,x='Outlet_Establishment_Year',y='Item_Outlet_Sales',hue='Item_Outlet_Sales', palette='RdYlBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sales were roughly the same in each year, with only 1998 having unusually low sales.<br>\n",
    "Referring to the frequency in 1998, the number of outlets established in 1998 was very small, resulting in very low sales.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlet_Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4,4))\n",
    "sns.barplot(df, x='Outlet_Size', y='Item_Outlet_Sales', ci=None, palette='RdYlBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.swarmplot(df,x='Outlet_Size',y='Item_Outlet_Sales',hue='Item_Outlet_Sales', palette='RdYlBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mid-sized stores have the highest sales.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlet_Location_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4,4))\n",
    "sns.barplot(df, x='Outlet_Location_Type', y='Item_Outlet_Sales', ci=None, palette='RdYlBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.swarmplot(df,x='Outlet_Location_Type',y='Item_Outlet_Sales',hue='Item_Outlet_Sales', palette='RdYlBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tier 2 location type has the highest sales, with little difference from the second place.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlet_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,4))\n",
    "sns.barplot(df, x='Outlet_Type', y='Item_Outlet_Sales', ci=None, palette='RdYlBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.swarmplot(df,x='Outlet_Type',y='Item_Outlet_Sales',hue='Item_Outlet_Sales', palette='RdYlBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The proportion of supermarket Type1 is very high, but the item mean sales is not very high.<br>\n",
    "The one with the highest sales is supermarket Type3.<br>\n",
    "Maybe supermarket Type3 is a high-end imported supermarket, and supermarket Type1 is an affordable ordinary supermarket.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(exclude='object').corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Correlation between categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features=df.select_dtypes(include='object').columns\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "import researchpy as rp\n",
    "def cramer_v(df , list_cat) :\n",
    "    df_result = pd.DataFrame(columns=['Category1', 'Category2', 'pvalue', 'Cramer'])    \n",
    "    for i in range(len(list_cat)) :\n",
    "        for j in range(i+1,len(list_cat)) :\n",
    "            coli = df[list_cat[i]]\n",
    "            colj = df[list_cat[j]]\n",
    "            crosstab , test_result=rp.crosstab(coli,colj,test='chi-square')\n",
    "            pvalue = test_result.loc[1]\n",
    "            cramer = test_result.loc[2]\n",
    "            new_result = pd.DataFrame({'Category1': [list_cat[i]], 'Category2': [list_cat[j]], 'pvalue': [pvalue[1]], 'Cramer': [cramer[1]]})\n",
    "            df_result = pd.concat([df_result, new_result], ignore_index=True)\n",
    "    return df_result\n",
    "\n",
    "result = cramer_v(df ,list(cat_features))\n",
    "result.sort_values(by='Cramer' , ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### as we can see here Item_Identifier and Outlet_Identifier have highly strong relation with other features , i drop both of them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Item_Identifier','Outlet_Identifier'] , axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features=df.select_dtypes(include='object').columns\n",
    "result = cramer_v(df ,list(cat_features))\n",
    "result.sort_values(by='Cramer' , ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='Outlet_Establishment_Year' , axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features=df.select_dtypes(include='object').columns\n",
    "result = cramer_v(df ,list(cat_features))\n",
    "result.sort_values(by='Cramer' , ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since the sample size is large enough (≥30), I can rely on the Central Limit Theorem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneity of Variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "features = ['Item_Weight', 'Item_Visibility', 'Item_MRP']\n",
    "levene_results = []\n",
    "\n",
    "for feature in features:\n",
    "    group1 = df[feature][df['Item_Outlet_Sales'] == 0]\n",
    "    group2 = df[feature][df['Item_Outlet_Sales'] == 1]\n",
    "    \n",
    "    stat, p_value = stats.levene(group1, group2)\n",
    "    levene_results.append((feature, stat, p_value))\n",
    "\n",
    "levene_df = pd.DataFrame(levene_results, columns=['Feature', 'Statistic', 'P-value'])\n",
    "print(levene_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since p-value= is less than 0.05, we can reject the null hypothesis and conclude that they have significant difference in their variances.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Item_Weight', 'Item_Visibility', 'Item_MRP']\n",
    "ttest_results = []\n",
    "\n",
    "for feature in features:\n",
    "    group1 = df[feature][df['Item_Outlet_Sales'] == 0]\n",
    "    group2 = df[feature][df['Item_Outlet_Sales'] == 1]\n",
    "    \n",
    "    t_statistic, p_value = stats.ttest_ind(group1, group2, equal_var=False)\n",
    "    ttest_results.append((feature, t_statistic, p_value))\n",
    "    \n",
    "ttest_df = pd.DataFrame(ttest_results, columns=['Feature', 'T-Statistic', 'P-value'])\n",
    "print(ttest_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since the p-values for each feature are less than 0.05, we can reject the null hypothesis and conclude that there have significant association between them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Before encoding all features , i first encode all except Outlet_size to use random forrest classifier to impute missing data and then encode Outlet_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded=df.drop(columns='Outlet_Size',axis=1)\n",
    "df_encoded=pd.get_dummies(df_encoded,drop_first=True)\n",
    "# df=df_test.copy()\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat df_encoded with uncoded outlet_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded2 = pd.concat([df_encoded,df.Outlet_Size],axis=1)\n",
    "df_encoded2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded2.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Outlet_Size using RandomForrestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Assuming 'categorical_feature' is the column you want to impute\n",
    "\n",
    "\n",
    "# Separate data into complete and incomplete\n",
    "complete_data = df_encoded2.dropna(subset=['Outlet_Size'])\n",
    "incomplete_data = df_encoded2[df_encoded2['Outlet_Size'].isna()]\n",
    "\n",
    "# Features and target\n",
    "X = complete_data.drop(['Outlet_Size'], axis=1)\n",
    "y = complete_data['Outlet_Size']\n",
    "\n",
    "# Train RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# Predict missing values\n",
    "predicted_values = classifier.predict(incomplete_data.drop(['Outlet_Size'], axis=1))\n",
    "\n",
    "# Fill in the missing values\n",
    "df_encoded2.loc[df_encoded2['Outlet_Size'].isna(), 'Outlet_Size'] = predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_encoded2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Outlet_Size.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now encode outlet_size :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded=df.copy()\n",
    "df_encoded=pd.get_dummies(df_encoded,drop_first=True)\n",
    "# df=df_test.copy()\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_encoded.astype(int)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_encoded.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outlier: LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "def outlier_detection(df):\n",
    "    x = df.drop(\"Item_Outlet_Sales\", axis=1)\n",
    "    y = df.Item_Outlet_Sales\n",
    "    lof = LocalOutlierFactor(n_neighbors = 10, metric= \"euclidean\") #or manhattan\n",
    "\n",
    "    res = lof.fit_predict(x)\n",
    "    print(res)\n",
    "  \n",
    "    x_o = x[res != -1]\n",
    "    y_o = y[res != -1]\n",
    "    df = pd.concat([x_o, y_o], axis=1)\n",
    "    return df\n",
    "\n",
    "df_cleaned= outlier_detection(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of removed observations : ' ,df.shape[0]-df_cleaned.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define X and Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('Item_Outlet_Sales',axis=1)\n",
    "y=df.Item_Outlet_Sales\n",
    "X.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3 , shuffle=True,random_state=1234)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train_Scaled = pd.DataFrame(sc.fit_transform(X_train), columns=X_train.columns)\n",
    "X_train_Scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_Scaled = pd.DataFrame(sc.transform(X_test),columns=X_test.columns)\n",
    "X_test_Scaled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train_Scaled\n",
    "X_test=X_test_Scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train17=X_train.copy()\n",
    "X_test17 = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation scores for the model. \n",
    "from sklearn.model_selection import cross_val_score\n",
    "crossvalidation = cross_val_score(linear_regressor, X_train, y_train, cv=5, n_jobs=-1)\n",
    "crossvalidation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pedictions on the train set. \n",
    "y_pred = linear_regressor.predict(X_train)\n",
    "y_pred_test = linear_regressor.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics value \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "r2_val = r2_score(y_train,y_pred)\n",
    "r2_adj_val=1 - (((len(X_train.index) - 1) / (len(X_train.index) - len(X_train.columns) - 1)) * (1 - r2_score(y_train,y_pred)))\n",
    "rmse_error = mean_squared_error (y_train, y_pred, squared = False)\n",
    "print (\"R2 score for the model is :\",r2_val )\n",
    "print(\"Adjusted_R2 for the model is :\",r2_adj_val)\n",
    "print (\"RMSE error for the model is :\",rmse_error )\n",
    "\n",
    "y_pred_test = linear_regressor.predict(X_test)\n",
    "r2_val_test = r2_score(y_test,y_pred_test)\n",
    "r2_adj_val_test=1 - (((len(X_test.index) - 1) / (len(X_test.index) - len(X_test.columns) - 1)) * (1 - r2_score(y_test,y_pred_test)))\n",
    "\n",
    "rmse_error_test = mean_squared_error (y_test, y_pred_test, squared = False)\n",
    "print (\"R2 score (test) for the model is :\",r2_val_test )\n",
    "print(\"Adjusted_R2(test) for the model is :\",r2_adj_val_test)\n",
    "print (\"RMSE error (test) for the model is :\",rmse_error_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an entry to store the data\n",
    "def store_results(name, y_pred, y_train, X_train, y_pred_test, y_test, X_test, model, folds, norm, Alpha):\n",
    "\n",
    "    \"\"\"    creates and entry to add to the resuts dataframe\n",
    "    name: name of the model \n",
    "    y_pred: predicted y \n",
    "    y_train: true value of y\n",
    "    X_train: features \n",
    "    model: model to be fit\n",
    "    folds: number of folds in cv\n",
    "    norm: L1 or L2\n",
    "    Alpha: Value of regularization parameter\"\"\"\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_train) # predictions on the train set.\n",
    "    r2_val = r2_score(y_train,y_pred)\n",
    "    r2_adj_val=1 - (((len(X_train.index) - 1) / (len(X_train.index) - len(X_train.columns) - 1)) * (1 - r2_score(y_train,y_pred)))\n",
    "    rmse_error = mean_squared_error (y_train, y_pred, squared = False)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    r2_val_test = r2_score(y_test,y_pred_test)\n",
    "    r2_adj_val_test=1 - (((len(X_test.index) - 1) / (len(X_test.index) - len(X_test.columns) - 1)) * (1 - r2_score(y_test,y_pred_test)))\n",
    "    rmse_error_test = mean_squared_error (y_test, y_pred_test, squared = False)\n",
    "    crossvalidation = cross_val_score(model, X_train, y_train, cv=folds, n_jobs=-1)\n",
    "    \n",
    "    entry = {'Model': [name],\n",
    "          'Regularization' : [norm],\n",
    "          'Alpha_value' : [Alpha],\n",
    "         'R2Score': [r2_val],\n",
    "         'Adjusted_R2Score': [r2_adj_val],\n",
    "         'RMSE': [rmse_error],\n",
    "         'R2Score_Test': [r2_val_test],\n",
    "         'Adjusted_R2Score_test': [r2_adj_val_test],\n",
    "         'RMSE_Test': [rmse_error_test],\n",
    "          'CrossVal_Mean(r2)': [crossvalidation.mean()],           \n",
    "          'CrossVal1(r2)': [crossvalidation[0]],\n",
    "          'CrossVal2(r2)': [crossvalidation[1]],\n",
    "          'CrossVal3(r2)': [crossvalidation[2]],\n",
    "          'CrossVal4(r2)': [crossvalidation[3]],\n",
    "          'CrossVal5(r2)': [crossvalidation[4]],\n",
    "          }\n",
    "\n",
    "\n",
    "    result = pd.DataFrame(entry)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= LinearRegression()\n",
    "temp = store_results(\"With 17 Features\", y_pred, y_train,X_train, y_pred_test, y_test,X_test, linear_regressor, 5, np.nan, np.nan)\n",
    "temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes=temp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RFE \n",
    "from sklearn.feature_selection import RFE\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# List for holding values \n",
    "i_list = []\n",
    "r2_list = []\n",
    "r2_adj_list=[]\n",
    "rmse_list = []\n",
    "cross_val_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "for i in range(3, 18, 1):                             # Performing RFE with 1 step jumps\n",
    "    rfe = RFE(model , n_features_to_select = i )        # running RFE with i variable output.\n",
    "    rfe.fit(X_train, y_train)                           # Fit the RFE model \n",
    "    col = X_train.columns[rfe.support_]                 # identify the columns selected by the RFE model\n",
    "    linear_regressor = LinearRegression()             \n",
    "    linear_regressor.fit(X_train[col],y_train)          # Train a new linear model with RFE selected columns \n",
    "    crossvalidation = cross_val_score(linear_regressor, X_train[col], y_train, cv=5, n_jobs=-1).mean() # Calculate the cross vall scores for the new model  \n",
    "    # Predictions on the train set. \n",
    "    y_pred = linear_regressor.predict(X_train[col])\n",
    "    r2_val = r2_score(y_train,y_pred)                    # find R-squared,adjusted R-squared  and RMSE for the new model\n",
    "    r2_adj_val=1 - (((len(X_train[col].index) - 1) / (len(X_train[col].index) - len(X_train[col].columns) - 1)) * (1 - r2_score(y_train,y_pred)))\n",
    "    rmse_error = mean_squared_error (y_train, y_pred, squared = False)\n",
    "    # maintain a list for performance to analyse in future\n",
    "    i_list.append(i)\n",
    "    r2_list.append(r2_val)\n",
    "    r2_adj_list.append(r2_adj_val)\n",
    "    rmse_list.append(rmse_error)\n",
    "    cross_val_list.append(crossvalidation)\n",
    "    # print the outputs \n",
    "    print (i)\n",
    "    print (\"R2 score for the model is :\",r2_val )\n",
    "    print(\"Adjusted_R2 :\",r2_adj_val)\n",
    "    print (\"RMSE error for the model is :\",rmse_error )\n",
    "    print (\"Mean Cross Validation Score (r2) :\",crossvalidation )\n",
    "    print (\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation of r2 with number of features \n",
    "import seaborn as sns\n",
    "sns.lineplot( x=i_list, y=r2_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation of Adjusted r2 with number of features \n",
    "sns.lineplot( x=i_list, y=r2_adj_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation of RMSE with increasing number of features \n",
    "sns.lineplot( x=i_list, y=rmse_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "i = 4                                              # Performing RFE with 3 step jumps\n",
    "rfe = RFE(model , n_features_to_select = i )        # running RFE with i variable output.\n",
    "rfe.fit(X_train, y_train)                           # Fit the RFE model \n",
    "col = X_train.columns[rfe.support_]                 # identify the columns slected by the RFE model\n",
    "linear_regressor = LinearRegression()             \n",
    "linear_regressor.fit(X_train[col],y_train)          # Train a new linear model with RFE slected columns \n",
    "crossvalidation = cross_val_score(linear_regressor, X_train[col], y_train, cv=5, n_jobs=-1).mean() # Calculate the cross vall scores for the new model  \n",
    "# predictions on the train set. \n",
    "y_pred = linear_regressor.predict(X_train[col])\n",
    "r2_val = r2_score(y_train,y_pred)                    # find R-squared,adjusted R-squared  and RMSE for the new model\n",
    "r2_adj_val=1 - (((len(X_train[col].index) - 1) / (len(X_train[col].index) - len(X_train[col].columns) - 1)) * (1 - r2_score(y_train,y_pred)))\n",
    "rmse_error = mean_squared_error (y_train, y_pred, squared = False)\n",
    "# maintain a list for performance to analyse in future\n",
    "i_list.append(i)\n",
    "r2_list.append(r2_val)\n",
    "r2_adj_list.append(r2_adj_val)\n",
    "rmse_list.append(rmse_error)\n",
    "cross_val_list.append(crossvalidation)\n",
    "# print the outputs \n",
    "print (i)\n",
    "print (\"R2 score for the model is :\",r2_val )\n",
    "print(\"Adjusted_R2 :\",r2_adj_val)\n",
    "print (\"RMSE error for the model is :\",rmse_error )\n",
    "print (\"Mean Cross Validation Score (r2) :\",crossvalidation )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_cols = col\n",
    "col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns which are droped\n",
    "set(X_train.columns)-set(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As observed, removing these features has a negligible impact on the model score, with only a slight change by a few hundredths. Consequently, we have opted to retain all the columns, as we are not persuaded that their removal would be beneficial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "     \n",
    "# before droping lets analyze VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the rest of the columns form both train and test sets. \n",
    "# X_train = X_train[rfe_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = X_train.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection with VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for the VIF values of the feature variables. \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "print(vif.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to keep features with VIF<5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop ([\"Outlet_Size_Small\"], axis =1)\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "print(vif.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vif_calc (x_train):\n",
    "#   '''\n",
    "#   x_train = Training feature set\n",
    "#   '''\n",
    "#   vif = pd.DataFrame()\n",
    "#   vif['Features'] = x_train.columns\n",
    "#   vif['VIF'] = [variance_inflation_factor(x_train.values, i) for i in range(x_train.shape[1])]\n",
    "#   vif['VIF'] = round(vif['VIF'], 2)\n",
    "#   vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "#   #print(vif.to_string())\n",
    "#   topval = vif.head(1)\n",
    "#   return topval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train1 = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vif_calc(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape , X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[X_train.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape , X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X_train.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_correlation(df1, treshold=0.6):\n",
    "    pairs = pd.DataFrame(columns=['feature1','feature2','value'])\n",
    "    cm = df1.corr() #correlation matrix\n",
    "    np.fill_diagonal(cm.values, 0) # set diagonal to 0 \n",
    "    corr = [(cm.index[x], cm.columns[y], cm.iloc[x,y]) for x, y in zip(*np.where(abs(np.tril(cm)) > treshold))] # create couple (feature1, feature2, value)\n",
    "    for couple in corr:\n",
    "        feature1, feature2, value = couple\n",
    "        #print(f'{feature1} and {feature2} are strongly correlated (treshold = {treshold}) (value = {value})')\n",
    "        entry = {'feature1': [feature1], 'feature2' : [feature2], 'value' : [value]}\n",
    "        temp = pd.DataFrame(entry)\n",
    "        pairs = pd.concat([pairs, temp], ignore_index=True) \n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_correlation(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns='Outlet_Type_Supermarket Type1',axis=1 , inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test[X_train.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape , X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_correlation(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After assessing the Variance Inflation Factor (VIF) and intercorrelation among the features, we have identified and retained a subset of 15 features. Subsequently, I created separate training and testing datasets using these 15 features, with the intention of applying a model to this refined dataset in subsequent steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train15 = X_train.copy()\n",
    "X_test15 = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is no any strong relation between pair features with threshhold >0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets back to check with RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for holding values \n",
    "i_list = []\n",
    "r2_list = []\n",
    "r2_adj_list=[]\n",
    "rmse_list = []\n",
    "cross_val_list = []\n",
    "\n",
    "model = LinearRegression()\n",
    "for i in range(3, 16, 1):                             # Performing RFE with 3 step jumps\n",
    "    rfe = RFE(model , n_features_to_select = i )        # running RFE with i variable output.\n",
    "    rfe.fit(X_train, y_train)                           # Fit the RFE model \n",
    "    col = X_train.columns[rfe.support_]                 # identify the columns selected by the RFE model\n",
    "    linear_regressor = LinearRegression()             \n",
    "    linear_regressor.fit(X_train[col],y_train)          # Train a new linear model with RFE selected columns \n",
    "    crossvalidation = cross_val_score(linear_regressor, X_train[col], y_train, cv=5, n_jobs=-1).mean() # Calculate the cross vall scores for the new model  \n",
    "    # Predictions on the train set. \n",
    "    y_pred = linear_regressor.predict(X_train[col])\n",
    "    r2_val = r2_score(y_train,y_pred)                    # find R-squared,adjusted R-squared  and RMSE for the new model\n",
    "    r2_adj_val=1 - (((len(X_train[col].index) - 1) / (len(X_train[col].index) - len(X_train[col].columns) - 1)) * (1 - r2_score(y_train,y_pred)))\n",
    "    rmse_error = mean_squared_error (y_train, y_pred, squared = False)\n",
    "    # maintain a list for performance to analyse in future\n",
    "    i_list.append(i)\n",
    "    r2_list.append(r2_val)\n",
    "    r2_adj_list.append(r2_adj_val)\n",
    "    rmse_list.append(rmse_error)\n",
    "    cross_val_list.append(crossvalidation)\n",
    "    # print the outputs \n",
    "    print (i)\n",
    "    print (\"R2 score for the model is :\",r2_val )\n",
    "    print(\"Adjusted_R2 :\",r2_adj_val)\n",
    "    print (\"RMSE error for the model is :\",rmse_error )\n",
    "    print (\"Mean Cross Validation Score (r2) :\",crossvalidation )\n",
    "    print (\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation of r2 with number of features \n",
    "import seaborn as sns\n",
    "sns.lineplot( x=i_list, y=r2_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After applying Recursive Feature Elimination (RFE), we did not observe a significant increase in the model score. Consequently, we have decided to proceed with using all the features in our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 : Linear regression with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's fit a linear regression model!\n",
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(X_train,y_train)\n",
    "\n",
    "# Cross validation scores for the model. \n",
    "crossvalidation = cross_val_score(linear_regressor, X_train, y_train, cv=5, n_jobs=-1)\n",
    "crossvalidation\n",
    "\n",
    "# pedictions on the train set. \n",
    "y_pred = linear_regressor.predict(X_train)\n",
    "y_pred_test = linear_regressor.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics value \n",
    "r2_val = r2_score(y_train,y_pred)\n",
    "r2_adj_val=1 - (((len(X_train.index) - 1) / (len(X_train.index) - len(X_train.columns) - 1)) * (1 - r2_score(y_train,y_pred)))\n",
    "rmse_error = mean_squared_error (y_train, y_pred, squared = False)\n",
    "print (\"R2 score for the model is :\",r2_val )\n",
    "print(\"Adjusted_R2 for the model is :\",r2_adj_val)\n",
    "print (\"RMSE error for the model is :\",rmse_error )\n",
    "\n",
    "y_pred_test = linear_regressor.predict(X_test)\n",
    "r2_val_test = r2_score(y_test,y_pred_test)\n",
    "r2_adj_val_test=1 - (((len(X_test.index) - 1) / (len(X_test.index) - len(X_test.columns) - 1)) * (1 - r2_score(y_test,y_pred_test)))\n",
    "\n",
    "rmse_error_test = mean_squared_error (y_test, y_pred_test, squared = False)\n",
    "print (\"R2 score (test) for the model is :\",r2_val_test )\n",
    "print(\"Adjusted_R2(test) for the model is :\",r2_adj_val_test)\n",
    "print (\"RMSE error (test) for the model is :\",rmse_error_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= LinearRegression()\n",
    "temp = store_results(\"With 15 Features(VIF and intercorralation)\", y_pred, y_train,X_train, y_pred_test, y_test,X_test, linear_regressor, 5, np.nan, np.nan)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n",
    "outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Elimination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression model using OLS\n",
    "import statsmodels.api as sm\n",
    "X1 = sm.add_constant(X_train)\n",
    "ols = sm.OLS(y_train,X1)\n",
    "lr = ols.fit()\n",
    "\n",
    "print(lr.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape , y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward feature elimination\n",
    "maxp = lr.pvalues.max()\n",
    "while(maxp > 0.05):\n",
    "    print(f\"Adjuste R-Square is {lr.rsquared_adj}\")\n",
    "    print(f\"{lr.pvalues.idxmax()} with p-value= {maxp} was dropped\\n\")\n",
    "    X1.drop(lr.pvalues.idxmax(),axis=1,inplace=True) \n",
    "    ols = sm.OLS(y_train,X1)\n",
    "    lr = ols.fit()\n",
    "    maxp = lr.pvalues.max()\n",
    "print(lr.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.drop('const',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X1.copy()\n",
    "\n",
    "X_test = X_test[X_train.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = store_results(\"After Backward Elimination(7Features)\", y_pred, y_train,X_train, y_pred_test, y_test,X_test, model, 5, np.nan, np.nan)\n",
    "temp \n",
    "\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the Intercept\n",
    "print(model.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coefficient\n",
    "coeff_df = pd.DataFrame(model.coef_.T, X_test.columns, columns=['Coefficient'])\n",
    "print('coeff=',coeff_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred, edgecolor='black')\n",
    "plt.xlabel(\"y_test\")\n",
    "plt.ylabel(\"y_pred\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE : \n",
    "(abs(y_test-y_pred)).describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model with ridge (L2) regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train17.shape , X_test17.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train17.copy()\n",
    "X_test=X_test17.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "ridge_model_15 = linear_model.Ridge(alpha = 0.01, random_state=42)\n",
    "ridge_model = linear_model.Ridge(alpha = 0.01, random_state=42)\n",
    "\"\"\"\n",
    "Linear least squares with l2 regularization.\n",
    "\n",
    "Minimizes the objective function::\n",
    "\n",
    "||y - Xw||^2_2 + alpha * ||w||^2_2\n",
    "\"\"\"\n",
    "ridge_model.fit (X_train, y_train)\n",
    "ridge_model_15.fit (X_train15, y_train)\n",
    "\n",
    "# predictions on the train set. \n",
    "y_pred= ridge_model.predict(X_train)\n",
    "y_pred_test = ridge_model.predict(X_test)\n",
    "\n",
    "y_pred_15 = ridge_model_15.predict(X_train15)\n",
    "y_pred_test_15 = ridge_model_15.predict(X_test15)\n",
    "\n",
    "temp = store_results(\"Ridge-1 with 17 features\", y_pred, y_train,X_train, y_pred_test, y_test,X_test, ridge_model, 5, \"L2\", 0.01)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n",
    "temp = store_results(\"Ridge-1 with 15 features\", y_pred_15, y_train,X_train15, y_pred_test_15, y_test,X_test15, ridge_model_15, 5, \"L2\", 0.01)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes2=outcomes.copy()\n",
    "for i in [0.0001, 0.0005, 0.001, 0.005, 0.05, 0.1, 0.5, 1, 5]:\n",
    "  ridge_model = linear_model.Ridge(alpha = i, random_state=42)\n",
    "  ridge_model.fit (X_train, y_train)\n",
    "\n",
    "  # predictions on the train set. \n",
    "  y_pred = ridge_model.predict(X_train)\n",
    "  y_pred_test = ridge_model.predict(X_test)\n",
    "  temp = store_results(\"Ridge with 17 features \", y_pred, y_train,X_train, y_pred_test, y_test,X_test, ridge_model, 5, \"L2\", i)\n",
    "  outcomes2 = pd.concat([outcomes2, temp], ignore_index=True)\n",
    "\n",
    "outcomes2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes2=outcomes.copy()\n",
    "for i in [0.0001, 0.0005, 0.001, 0.005, 0.05, 0.1, 0.5, 1, 5]:\n",
    "  ridge_model = linear_model.Ridge(alpha = i, random_state=42)\n",
    "  ridge_model.fit (X_train15, y_train)\n",
    "\n",
    "  # predictions on the train set. \n",
    "  y_pred = ridge_model.predict(X_train15)\n",
    "  y_pred_test = ridge_model.predict(X_test15)\n",
    "  temp = store_results(\"Ridge 15 features \", y_pred, y_train,X_train15, y_pred_test, y_test,X_test15, ridge_model, 5, \"L2\", i)\n",
    "  outcomes2 = pd.concat([outcomes2, temp], ignore_index=True)\n",
    "\n",
    "outcomes2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As observed, varying the alpha parameter over a wide range did not result in any discernible increase in the model scores. The scores remained relatively consistent across the tested alpha values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Ridge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "\n",
    "ridge_cv = RidgeCV(alphas = list(np.arange(1, 20, 0.2)))\n",
    "ridge_cv_15 = RidgeCV(alphas = list(np.arange(1, 20, 0.2)))\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "ridge_cv_15.fit(X_train15, y_train)\n",
    "# what alpha value did the algorithm choose\n",
    "alpha = ridge_cv.alpha_\n",
    "alpha_15 = ridge_cv_15.alpha_\n",
    "alpha , alpha_15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_opti = Ridge(alpha = alpha)\n",
    "ridge_opti.fit(X_train, y_train)\n",
    "\n",
    "ridge_opti_15 = Ridge(alpha = alpha)\n",
    "ridge_opti_15.fit(X_train15, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape , X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ridge_opti.predict(X_train)\n",
    "y_pred_test = ridge_opti.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_15 = ridge_opti_15.predict(X_train15)\n",
    "y_pred_test_15 = ridge_model_15.predict(X_test15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = store_results(\"Ridge-optimised with 17 features \", y_pred, y_train,X_train, y_pred_test, y_test,X_test, ridge_opti, 5, \"L2\", alpha)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = store_results(\"Ridge-optimised with 15 features \", y_pred_15, y_train,X_train15, y_pred_test_15, y_test,X_test15, ridge_opti_15, 5, \"L2\", alpha)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_opti.coef_  , ridge_opti_15.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with Lasso (L1) Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = linear_model.Lasso(alpha = 0.01, random_state=42)\n",
    "lasso_model.fit (X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model15 = linear_model.Lasso(alpha = 0.01, random_state=42)\n",
    "lasso_model15.fit (X_train15, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on the train set. \n",
    "y_pred = lasso_model.predict(X_train)\n",
    "y_pred_test = lasso_model.predict(X_test)\n",
    "\n",
    "y_pred15 = lasso_model15.predict(X_train15)\n",
    "y_pred_test_15 = lasso_model15.predict(X_test15)\n",
    "\n",
    "temp = store_results(\"Lasso with 17 features \", y_pred, y_train,X_train, y_pred_test, y_test,X_test, lasso_model, 5, \"L1\", 0.01)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n",
    "temp = store_results(\"Lasso with 15 features \", y_pred15, y_train,X_train15, y_pred_test_15, y_test,X_test15, lasso_model15, 5, \"L1\", 0.01)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n",
    "outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model.coef_ , lasso_model15.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes2=outcomes.copy()\n",
    "for i in [0.0001, 0.0005, 0.001, 0.005, 0.05, 0.1, 0.5, 1, 5, 10, 15, 20]:\n",
    "  lasso_model = linear_model.Lasso(alpha = i, random_state=42)\n",
    "  lasso_model.fit (X_train, y_train)\n",
    "\n",
    "  # pedictions on the train set. \n",
    "  y_pred = lasso_model.predict(X_train)\n",
    "  y_pred_test = lasso_model.predict(X_test)\n",
    "\n",
    "  # Count the number of zeors in the coeff list. \n",
    "  coeflist = list(lasso_model.coef_)\n",
    "  zeros = list(lasso_model.coef_).count(0)\n",
    "  print (i)\n",
    "  print (\"The number of zero coeff in the model are :\", zeros)\n",
    "  print (\"=======================================================\")\n",
    "\n",
    "  temp = store_results(\"Lasso\", y_pred, y_train,X_train, y_pred_test, y_test,X_test, lasso_model, 5, \"L1\", i)\n",
    "#   outcomes2 = outcomes2.append (temp)\n",
    "  outcomes2 = pd.concat([outcomes2, temp], ignore_index=True)\n",
    "\n",
    "outcomes2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using a discrete set of alpha values does not significantly impact the outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "\n",
    "lasso_cv = LassoCV(alphas = None, cv = 5, max_iter = 100000)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "lasso_cv15 = LassoCV(alphas = None, cv = 5, max_iter = 100000)\n",
    "lasso_cv15.fit(X_train15, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what alpha value did the algorithm choose\n",
    "alpha = lasso_cv.alpha_\n",
    "alpha15 = lasso_cv15.alpha_\n",
    "alpha,alpha15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_opti = Lasso(alpha = alpha)\n",
    "lasso_opti.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_opti15 = Lasso(alpha = alpha15)\n",
    "lasso_opti15.fit(X_train15, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lasso_opti.predict(X_train)\n",
    "y_pred_test = lasso_opti.predict(X_test)\n",
    "temp = store_results(\"Lasso-optimised with 17 features \", y_pred, y_train,X_train, y_pred_test, y_test,X_test, lasso_opti, 5, \"L1\", alpha)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n",
    "\n",
    "y_pred15 = lasso_opti15.predict(X_train15)\n",
    "y_pred_test15 = lasso_opti15.predict(X_test15)\n",
    "temp = store_results(\"Lasso-optimised with 15 features \", y_pred15, y_train,X_train15, y_pred_test15, y_test,X_test15, lasso_opti15, 5, \"L1\", alpha15)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_opti.coef_ ,lasso_opti15.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5: Random Forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model_rf = RandomForestRegressor(random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "feature_importances = model_rf.feature_importances_\n",
    "features = X_train.columns\n",
    "df = pd.DataFrame({'features': features, 'importance': feature_importances})\n",
    "df.sort_values(by='importance', ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.importance > 0.005]\n",
    "rf_cols = []\n",
    "for col in list(X_train.columns):\n",
    "  if col in list(df.features):\n",
    "    rf_cols.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestRegressor( random_state=42)\n",
    "model_rf.fit(X_train[rf_cols], y_train)\n",
    "len(rf_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rf = model_rf.predict(X_train[rf_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rf_test = model_rf.predict(X_test[rf_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = store_results(\"RF with 17 features\", predict_rf, y_train,X_train[rf_cols], predict_rf_test, y_test,X_test[rf_cols], model_rf, 5, np.nan, np.nan)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n",
    "outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF for 15 featues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf15 = RandomForestRegressor(random_state=42)\n",
    "model_rf15.fit(X_train15, y_train)\n",
    "\n",
    "feature_importances15 = model_rf15.feature_importances_\n",
    "features15 = X_train15.columns\n",
    "df15 = pd.DataFrame({'features': features15, 'importance': feature_importances15})\n",
    "df15.sort_values(by='importance', ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df15 = df15[df15.importance > 0.005]\n",
    "rf_cols15 = []\n",
    "for col in list(X_train15.columns):\n",
    "  if col in list(df15.features):\n",
    "    rf_cols15.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf15 = RandomForestRegressor( random_state=42)\n",
    "model_rf15.fit(X_train15[rf_cols15], y_train)\n",
    "len(rf_cols15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rf15 = model_rf15.predict(X_train15[rf_cols15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rf_test15 = model_rf15.predict(X_test15[rf_cols15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = store_results(\"RF with 15 features\", predict_rf15, y_train,X_train15[rf_cols15], predict_rf_test15, y_test,X_test15[rf_cols15], model_rf15, 5, np.nan, np.nan)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n",
    "outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression excels primarily in R2 Score and Adjusted R2 Score on the training set for both data sets with 15 and 17 features . However, other evaluation metrics for both the training and test datasets indicate comparatively lower performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "estimator = RandomForestRegressor()\n",
    "param_grid = { \n",
    "        \"n_estimators\"      : [5,8,10,12,15],\n",
    "        \"min_samples_split\" : [5,8,10,15,20],\n",
    "        \"max_depth\"         : [3,4,5]\n",
    "        }\n",
    "\n",
    "grid = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=5)\n",
    "\n",
    "grid.fit(X_train[rf_cols], y_train)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestRegressor(n_estimators = 8, min_samples_split = 5, max_depth= 5, n_jobs=-1)\n",
    "estimator.fit(X_train[rf_cols],y_train)\n",
    "y_predict_train = estimator.predict(X_train[rf_cols])\n",
    "y_predict_test = estimator.predict(X_test[rf_cols])\n",
    "temp = store_results(\"Tuned RF with 17 features \", y_predict_train, y_train,X_train[rf_cols], y_predict_test, y_test,X_test[rf_cols], estimator, 5, np.nan, np.nan)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n",
    "outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Random forest regressor for training set with 15 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_rf15.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator15 = RandomForestRegressor()\n",
    "param_grid15 = { \n",
    "        \"n_estimators\"      : [5,8,10,12,15],\n",
    "        \"min_samples_split\" : [5,8,10,15,20],\n",
    "        \"max_depth\"         : [3,4,5]\n",
    "        }\n",
    "\n",
    "grid15 = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=5)\n",
    "\n",
    "grid15.fit(X_train15[rf_cols15], y_train)\n",
    "\n",
    "print(grid15.best_score_)\n",
    "print(grid15.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator15 = RandomForestRegressor(n_estimators = 15, min_samples_split = 10, max_depth= 5, n_jobs=-1)\n",
    "estimator15.fit(X_train15[rf_cols15],y_train)\n",
    "y_predict_train15 = estimator15.predict(X_train15[rf_cols15])\n",
    "y_predict_test15 = estimator15.predict(X_test15[rf_cols15])\n",
    "temp = store_results(\"Tuned RF with 15 features \", y_predict_train15, y_train,X_train15[rf_cols15], y_predict_test15, y_test,X_test15[rf_cols15], estimator15, 5, np.nan, np.nan)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n",
    "outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF optimised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "param_grid = {'max_depth': [3, 5, 10],'min_samples_split': [2, 5, 10]}\n",
    "base_estimator = RandomForestRegressor(random_state=0)\n",
    "sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,factor=2, resource='n_estimators',max_resources=30).fit(X_train[rf_cols], y_train)\n",
    "sh.best_estimator_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_opti = RandomForestRegressor(n_estimators = 24, min_samples_split = 10, max_depth= 10,random_state=0, n_jobs=-1)\n",
    "RF_opti.fit(X_train[rf_cols],y_train)\n",
    "y_predict_train = RF_opti.predict(X_train[rf_cols])\n",
    "y_predict_test = RF_opti.predict(X_test[rf_cols])\n",
    "temp = store_results(\"RF_optimised with 17 features \", y_predict_train, y_train,X_train[rf_cols], y_predict_test, y_test,X_test[rf_cols], RF_opti, 5, np.nan, np.nan)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF optimised for 15 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid15 = {'max_depth': [3, 5, 10],'min_samples_split': [2, 5, 10]}\n",
    "base_estimator15 = RandomForestRegressor(random_state=0)\n",
    "sh15 = HalvingGridSearchCV(base_estimator15, param_grid15, cv=5,factor=2, resource='n_estimators',max_resources=30).fit(X_train15[rf_cols15], y_train)\n",
    "sh15.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RF_opti15 = RandomForestRegressor(n_estimators = 24, min_samples_split = 10, max_depth= 10,random_state=0, n_jobs=-1)\n",
    "RF_opti15.fit(X_train15[rf_cols15],y_train)\n",
    "y_predict_train15 = RF_opti15.predict(X_train15[rf_cols15])\n",
    "y_predict_test15 = RF_opti15.predict(X_test15[rf_cols15])\n",
    "temp = store_results(\"RF_optimised with 15 features \", y_predict_train15, y_train,X_train15[rf_cols15], y_predict_test15, y_test,X_test15[rf_cols15], RF_opti15, 5, np.nan, np.nan)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost Adaboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "Ada= AdaBoostRegressor(random_state=0)\n",
    "\n",
    "Ada.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_predict_train = Ada.predict(X_train)\n",
    "\n",
    "# Make predictions on the test set (assuming X_test is your test data)\n",
    "y_predict_test = Ada.predict(X_test)\n",
    "\n",
    "\n",
    "temp = store_results(\"AdaBoost wit 17 features \", y_predict_train, y_train,X_train, y_predict_test, y_test,X_test, Ada, 5, np.nan, np.nan)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Ada15= AdaBoostRegressor(random_state=0)\n",
    "\n",
    "Ada15.fit(X_train15, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_predict_train15 = Ada15.predict(X_train15)\n",
    "\n",
    "# Make predictions on the test set (assuming X_test is your test data)\n",
    "y_predict_test15 = Ada15.predict(X_test15)\n",
    "\n",
    "\n",
    "temp = store_results(\"AdaBoost wit 15 features \", y_predict_train15, y_train,X_train15, y_predict_test15, y_test,X_test15, Ada15, 5, np.nan, np.nan)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "param_grid={'n_estimators':range(10,110,10)}\n",
    "clf=GridSearchCV(AdaBoostRegressor(DecisionTreeRegressor(max_depth=4)),param_grid)\n",
    "clf.fit(X_train,y_train)\n",
    "clf.best_params_\n",
    "ADA_Tuned=clf.best_estimator_\n",
    "temp = store_results(\"Tuned AdaBoost with 17 features \", y_predict_train, y_train,X_train, y_predict_test, y_test,X_test, ADA_Tuned, 5, np.nan, np.nan)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid15={'n_estimators':range(10,110,10)}\n",
    "clf15=GridSearchCV(AdaBoostRegressor(DecisionTreeRegressor(max_depth=4)),param_grid)\n",
    "clf15.fit(X_train15,y_train)\n",
    "\n",
    "\n",
    "ADA_Tuned15=clf15.best_estimator_\n",
    "temp = store_results(\"Tuned AdaBoost with 15 features \", y_predict_train15, y_train,X_train15, y_predict_test15, y_test,X_test15, ADA_Tuned15, 5, np.nan, np.nan)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n",
    "outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_ , clf15.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "# Initialize and train the XGBoost regressor\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "\n",
    "\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "y_predict_train = xg_reg.predict(X_train)\n",
    "\n",
    "y_predict_test = xg_reg.predict(X_test)\n",
    "\n",
    "temp = store_results(\"XGBoost with 17 features \", y_predict_train, y_train,X_train, y_predict_test, y_test,X_test, xg_reg, 5, np.nan, np.nan)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n",
    "outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'alpha': [1, 5, 10],\n",
    "    'n_estimators': [10, 50, 100]\n",
    "}\n",
    "# Grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=xg_reg, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', cv=5, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Extract the best estimator\n",
    "XGBoost_Tuned = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_predict_train = XGBoost_Tuned.predict(X_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_predict_test = XGBoost_Tuned.predict(X_test)\n",
    "\n",
    "\n",
    "temp = store_results(\"Tuned XGBoost with 17 features\", y_predict_train, y_train,X_train, y_predict_test, y_test,X_test, XGBoost_Tuned, 5, np.nan, np.nan)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n",
    "outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg15 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "\n",
    "xg_reg15.fit(X_train15, y_train)\n",
    "\n",
    "y_predict_train15 = xg_reg15.predict(X_train15)\n",
    "\n",
    "y_predict_test15 = xg_reg15.predict(X_test15)\n",
    "\n",
    "temp = store_results(\"XGBoost with 15 features \", y_predict_train15, y_train,X_train15, y_predict_test15, y_test,X_test15, xg_reg15, 5, np.nan, np.nan)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n",
    "outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xg_reg15 = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'alpha': [1, 5, 10],\n",
    "    'n_estimators': [10, 50, 100]\n",
    "}\n",
    "# Grid search with 5-fold cross-validation\n",
    "grid_search15 = GridSearchCV(estimator=xg_reg, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', cv=5, verbose=1)\n",
    "\n",
    "grid_search15.fit(X_train15, y_train)\n",
    "\n",
    "best_params15 = grid_search15.best_params_\n",
    "\n",
    "# Extract the best estimator\n",
    "XGBoost_Tuned15 = grid_search15.best_estimator_\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_predict_train15 = XGBoost_Tuned15.predict(X_train15)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_predict_test15 = XGBoost_Tuned15.predict(X_test15)\n",
    "\n",
    "temp = store_results(\"Tuned XGBoost with 15 features\", y_predict_train15, y_train,X_train15, y_predict_test15, y_test,X_test15, XGBoost_Tuned15, 5, np.nan, np.nan)\n",
    "# outcomes = outcomes.append (temp)\n",
    "outcomes = pd.concat([outcomes, temp], ignore_index=True)\n",
    "\n",
    "outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting results :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(20, 12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting all models : \n",
    "    - Model performance comparison (Train and Test R-squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = outcomes['Model']\n",
    "train_r2 = outcomes['R2Score']\n",
    "test_r2 = outcomes['R2Score_Test']\n",
    "\n",
    "bar_height = 0.35\n",
    "\n",
    "y = np.arange(len(models))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "plt.barh(y - bar_height/2, train_r2, bar_height, label='Train R-squared', color='skyblue')\n",
    "\n",
    "plt.barh(y + bar_height/2, test_r2, bar_height, label='Test R-squared', color='lightcoral')\n",
    "\n",
    "plt.yticks(y, models)\n",
    "plt.ylabel('Model')\n",
    "plt.xlabel('R-squared')\n",
    "plt.title('Model performance comparison (Train and Test R-squared)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = outcomes['Model']\n",
    "train_adr2 = outcomes['Adjusted_R2Score']\n",
    "test_adr2 = outcomes['Adjusted_R2Score_test']\n",
    "\n",
    "bar_height = 0.35\n",
    "\n",
    "y = np.arange(len(models))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "plt.barh(y - bar_height/2, train_adr2, bar_height, label='Train Adjusted_R2Score', color='skyblue')\n",
    "\n",
    "plt.barh(y + bar_height/2, test_adr2, bar_height, label='Test Adjusted_R2Score', color='lightcoral')\n",
    "\n",
    "plt.yticks(y, models)\n",
    "plt.ylabel('Model')\n",
    "plt.xlabel('Adjusted_R2Score')\n",
    "plt.title('Model performance comparison (Train and Test Adjusted_R2Score)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = outcomes['Model']\n",
    "train_rmse = outcomes['RMSE']\n",
    "test_rmse = outcomes['RMSE_Test']\n",
    "\n",
    "bar_height = 0.35\n",
    "\n",
    "y = np.arange(len(models))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "plt.barh(y - bar_height/2, train_rmse, bar_height, label='Train RMSE', color='skyblue')\n",
    "\n",
    "plt.barh(y + bar_height/2, test_rmse, bar_height, label='Test RMSE', color='lightcoral')\n",
    "\n",
    "plt.yticks(y, models)\n",
    "plt.ylabel('Model Name')\n",
    "plt.xlabel('RMSE')\n",
    "plt.title('Model performance comparison (Train and Test RMSE)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting all model with 17 features  based on R2Score : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outcomes_sorted = outcomes[outcomes['Model'].str.lower().str.contains('17 features')].sort_values(by='R2Score', ascending=True)\n",
    "\n",
    "models = outcomes_sorted['Model']\n",
    "train_r2 = outcomes_sorted['R2Score']\n",
    "test_r2 = outcomes_sorted['R2Score_Test']\n",
    "\n",
    "bar_height = 0.35\n",
    "\n",
    "y = np.arange(len(models))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "plt.barh(y - bar_height/2, train_r2, bar_height, label='Train R-squared', color='skyblue')\n",
    "\n",
    "plt.barh(y + bar_height/2, test_r2, bar_height, label='Test R-squared', color='lightcoral')\n",
    "\n",
    "plt.yticks(y, models)\n",
    "plt.ylabel('Model')\n",
    "plt.xlabel('R-squared')\n",
    "plt.title('Model performance comparison (Train and Test R-squared)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting all model with 15 features  based on R2Score : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_sorted = outcomes[outcomes['Model'].str.lower().str.contains('15 features')].sort_values(by='R2Score', ascending=True)\n",
    "\n",
    "models = outcomes_sorted['Model']\n",
    "train_r2 = outcomes_sorted['R2Score']\n",
    "test_r2 = outcomes_sorted['R2Score_Test']\n",
    "\n",
    "bar_height = 0.35\n",
    "\n",
    "y = np.arange(len(models))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "plt.barh(y - bar_height/2, train_r2, bar_height, label='Train R-squared', color='skyblue')\n",
    "\n",
    "plt.barh(y + bar_height/2, test_r2, bar_height, label='Test R-squared', color='lightcoral')\n",
    "\n",
    "plt.yticks(y, models)\n",
    "plt.ylabel('Model')\n",
    "plt.xlabel('R-squared')\n",
    "plt.title('Model performance comparison (Train and Test R-squared) - 15 Features')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "outcomes_sorted = outcomes.sort_values(by='R2Score', ascending=True)\n",
    "\n",
    "plt.barh(data=outcomes_sorted,width='R2Score' ,y= 'Model', color='skyblue')\n",
    "plt.xlabel('R-squared')\n",
    "plt.title('Model performance comparison (R-squared)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: blue;\">Model Evaluation Summary</span>\n",
    "\n",
    "## <span style=\"color: green;\">Model 0: With 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.5556\n",
    "- Test R-squared (R2Score_Test): 0.5577\n",
    "- Comments: The model seems to have moderate predictive power on the training set, but it generalizes reasonably well to the test set as indicated by a similar R-squared.\n",
    "\n",
    "## <span style=\"color: green;\">Model 1: With 15 Features (VIF and Intercorrelation)</span>\n",
    "- Train R-squared (R2Score): 0.4512\n",
    "- Test R-squared (R2Score_Test): 0.4562\n",
    "- Comments: The model's performance is lower compared to the one with 17 features, and there might be room for improvement.\n",
    "\n",
    "## <span style=\"color: green;\">Model 2: After Backward Elimination (7 Features)</span>\n",
    "- Train R-squared (R2Score): 0.4503\n",
    "- Test R-squared (R2Score_Test): 0.4583\n",
    "- Comments: Similar to Model 1, but with fewer features. There is a risk of underfitting due to the simplicity of the model.\n",
    "\n",
    "## <span style=\"color: green;\">Model 3: Ridge-1 with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.5556\n",
    "- Test R-squared (R2Score_Test): 0.5577\n",
    "- Comments: Ridge regularization doesn't seem to significantly impact the model's performance.\n",
    "\n",
    "## <span style=\"color: green;\">Model 4: Ridge-1 with 15 Features</span>\n",
    "- Train R-squared (R2Score): 0.4512\n",
    "- Test R-squared (R2Score_Test): 0.4562\n",
    "- Comments: Similar to Model 1, Ridge regularization doesn't seem to improve performance.\n",
    "\n",
    "## <span style=\"color: green;\">Model 5: Ridge-optimized with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.5556\n",
    "- Test R-squared (R2Score_Test): 0.5576\n",
    "- Comments: The optimized Ridge model performs similarly to the basic Ridge model.\n",
    "\n",
    "## <span style=\"color: green;\">Model 6: Ridge-optimized with 15 Features</span>\n",
    "- Train R-squared (R2Score): 0.4512\n",
    "- Test R-squared (R2Score_Test): 0.4562\n",
    "- Comments: Similar to Model 4, the optimized Ridge model doesn't significantly improve performance.\n",
    "\n",
    "## <span style=\"color: green;\">Model 7: Lasso with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.5556\n",
    "- Test R-squared (R2Score_Test): 0.5577\n",
    "- Comments: Lasso regularization doesn't seem to impact the model's performance.\n",
    "\n",
    "## <span style=\"color: green;\">Model 8: Lasso with 15 Features</span>\n",
    "- Train R-squared (R2Score): 0.4512\n",
    "- Test R-squared (R2Score_Test): 0.4562\n",
    "- Comments: Similar to Model 1, Lasso regularization doesn't significantly improve performance.\n",
    "\n",
    "## <span style=\"color: green;\">Model 9: Lasso-optimized with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.5553\n",
    "- Test R-squared (R2Score_Test): 0.5578\n",
    "- Comments: The optimized Lasso model performs similarly to the basic Lasso model.\n",
    "\n",
    "## <span style=\"color: green;\">Model 10: Lasso-optimized with 15 Features</span>\n",
    "- Train R-squared (R2Score): 0.4512\n",
    "- Test R-squared (R2Score_Test): 0.4562\n",
    "- Comments: Similar to Model 8, the optimized Lasso model doesn't significantly improve performance.\n",
    "\n",
    "## <span style=\"color: green;\">Model 11: Random Forest (RF) with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.9147\n",
    "- Test R-squared (R2Score_Test): 0.5216\n",
    "- Comments: There's a significant difference between training and test R-squared, suggesting potential overfitting on the training set.\n",
    "\n",
    "## <span style=\"color: green;\">Model 12: RF with 15 Features</span>\n",
    "- Train R-squared (R2Score): 0.9057\n",
    "- Test R-squared (R2Score_Test): 0.4710\n",
    "- Comments: Similar to Model 11, overfitting is observed.\n",
    "\n",
    "## <span style=\"color: green;\">Model 13: Tuned RF with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.4589\n",
    "- Test R-squared (R2Score_Test): 0.4624\n",
    "- Comments: There's a small improvement over Model 11, but overfitting is still evident.\n",
    "\n",
    "## <span style=\"color: green;\">Model 14: Tuned RF with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.4807\n",
    "- Test R-squared (R2Score_Test): 0.4762\n",
    "- Comments: Overfitting is observed, and the model's performance on the test set is not significantly better than the untuned RF.\n",
    "\n",
    "## <span style=\"color: green;\">Model 15: Tuned RF with 15 Features</span>\n",
    "- Train R-squared (R2Score): 0.4762\n",
    "- Test R-squared (R2Score_Test): 0.4702\n",
    "- Comments: Similar to Model 14, overfitting is observed.\n",
    "\n",
    "## <span style=\"color: green;\">Model 16: RF-optimized with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.6758\n",
    "- Test R-squared (R2Score_Test): 0.5118\n",
    "- Comments: There's a significant difference between training and test R-squared, suggesting potential overfitting on the training set.\n",
    "\n",
    "## <span style=\"color: green;\">Model 17: RF-optimized with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.6537\n",
    "- Test R-squared (R2Score_Test): 0.5148\n",
    "- Comments: Similar to Model 16, overfitting is observed.\n",
    "\n",
    "## <span style=\"color: green;\">Model 18: RF-optimized with 15 Features</span>\n",
    "- Train R-squared (R2Score): 0.6537\n",
    "- Test R-squared (R2Score_Test): 0.5148\n",
    "- Comments: Similar to Models 16 and 17, overfitting is observed.\n",
    "\n",
    "## <span style=\"color: green;\">Model 19: AdaBoost with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.4568\n",
    "- Test R-squared (R2Score_Test): 0.4792\n",
    "- Comments: The model performs better on the test set compared to Models 11-18, but there might still be overfitting.\n",
    "\n",
    "## <span style=\"color: green;\">Model 20: AdaBoost with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.4568\n",
    "- Test R-squared (R2Score_Test): 0.4792\n",
    "- Comments: Similar to Model 19.\n",
    "\n",
    "## <span style=\"color: green;\">Model 21: AdaBoost with 15 Features</span>\n",
    "- Train R-squared (R2Score): 0.4104\n",
    "- Test R-squared (R2Score_Test): 0.4233\n",
    "- Comments: There is potential underfitting as the performance is lower compared to Models 19 and 20.\n",
    "\n",
    "## <span style=\"color: green;\">Model 22: Tuned AdaBoost with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.5592\n",
    "- Test R-squared (R2Score_Test): 0.5615\n",
    "- Comments: The model performs well on both training and test sets, indicating a good balance.\n",
    "\n",
    "## <span style=\"color: green;\">Model 23: Tuned AdaBoost with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.5119\n",
    "- Test R-squared (R2Score_Test): 0.5187\n",
    "- Comments: There's a slight difference between training and test R-squared, but the model's performance is reasonable.\n",
    "\n",
    "## <span style=\"color: green;\">Model 24: Tuned AdaBoost with 15 Features</span>\n",
    "- Train R-squared (R2Score): 0.4544\n",
    "- Test R-squared (R2Score_Test): 0.4573\n",
    "- Comments: Similar to Models 19 and 20, there might be overfitting.\n",
    "\n",
    "## <span style=\"color: green;\">Model 25: Tuned XGBoost with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.6072\n",
    "- Test R-squared (R2Score_Test): 0.5872\n",
    "- Comments: The model performs well on both training and test sets, indicating a good balance.\n",
    "\n",
    "## <span style=\"color: green;\">Model 26: XGBoost with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.0573\n",
    "- Test R-squared (R2Score_Test): 0.0336\n",
    "- Comments: There's a significant difference between training and test R-squared, indicating potential overfitting.\n",
    "\n",
    "## <span style=\"color: green;\">Model 27: XGBoost with 17 Features</span>\n",
    "- Train R-squared (R2Score): 0.0573\n",
    "- Test R-squared (R2Score_Test): 0.0336\n",
    "- Comments: Similar to Model 26.\n",
    "\n",
    "## <span style=\"color: green;\">Model 28: XGBoost with 15 Features</span>\n",
    "- Train R-squared (R2Score): -0.0854\n",
    "- Test R-squared (R2Score_Test): -0.0994\n",
    "- Comments: There's a significant difference between training and test R-squared, indicating potential overfitting.\n",
    "\n",
    "## <span style=\"color: green;\">Model 29: Tuned XGBoost with 15 Features</span>\n",
    "- Train R-squared (R2Score): 0.6198\n",
    "- Test R-squared (R2Score_Test): 0.5315\n",
    "- Comments: The model performs well on both training and test sets, indicating a good balance.\n",
    "\n",
    "# <span style=\"color: blue;\">Summary</span>\n",
    "\n",
    "Models like AdaBoost (22, 23) and Tuned XGBoost (25, 29) seem to strike a good balance between training and test performance.\n",
    "Models like Random Forest (11-18) and some versions of XGBoost (26-28) exhibit overfitting.\n",
    "Ridge and Lasso regularization don't seem to significantly impact the models.\n",
    "Feature selection strategies (Models 0-2) show mixed results, with potential for improvement.\n",
    "It's essential to further fine-tune hyperparameters and consider feature engineering to improve model performance and mitigate overfitting or underfitting. Additionally, cross-validation results should be carefully considered to ensure the robustness of the models.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection Considerations\n",
    "\n",
    "\n",
    "## Models with Good Generalization Performance:\n",
    "\n",
    "### Tuned XGBoost with 17 features (Model 25):\n",
    "- <span style=\"color: green;\">**High Train R-squared:** 0.6072</span>\n",
    "- <span style=\"color: green;\">**Test R-squared:** 0.5872</span>\n",
    "- <span style=\"color: green;\">**Comments:** This model shows good performance on both the training and test sets, indicating a good balance between fitting the training data and generalizing to new, unseen data.</span>\n",
    "\n",
    "### Tuned AdaBoost with 17 features (Model 22):\n",
    "- <span style=\"color: green;\">**Train R-squared:** 0.5592</span>\n",
    "- <span style=\"color: green;\">**Test R-squared:** 0.5615</span>\n",
    "- <span style=\"color: green;\">**Comments:** Like the XGBoost model, this AdaBoost model exhibits a good balance between training and test performance.</span>\n",
    "\n",
    "## Models with Potential Overfitting:\n",
    "\n",
    "### Random Forest (RF) with 17 features (Model 11):\n",
    "- <span style=\"color: green;\">**High Train R-squared:** 0.9147</span>\n",
    "- <span style=\"color: green;\">**Lower Test R-squared:** 0.5216</span>\n",
    "- <span style=\"color: green;\">**Comments:** There's a significant difference between training and test R-squared, suggesting overfitting on the training set.</span>\n",
    "\n",
    "### XGBoost with 17 features (Models 26 and 27):\n",
    "- <span style=\"color: green;\">**Train R-squared:** 0.0573</span>\n",
    "- <span style=\"color: green;\">**Test R-squared:** 0.0336</span>\n",
    "- <span style=\"color: green;\">**Comments:** These XGBoost models exhibit a significant difference between training and test R-squared, indicating potential overfitting.</span>\n",
    "\n",
    "## Consideration for Model Selection:\n",
    "\n",
    "Tuned XGBoost with 17 features (Model 25) seems to be a strong candidate as it exhibits good performance on both the training and test sets, suggesting it might generalize well to new data.\n",
    "\n",
    "It's crucial to consider the application's context and the importance of interpretability. More complex models might provide better predictive performance, but simpler models are often easier to interpret.\n",
    "\n",
    "Cross-validation results and other performance metrics (e.g., RMSE) should also be considered for a comprehensive evaluation.\n",
    "\n",
    "Further hyperparameter tuning or ensemble methods could potentially improve model performance.\n",
    "\n",
    "In conclusion, based on the provided information, Tuned XGBoost with 17 features (Model 25) appears to be a strong contender for the best model. However, the final decision should be made considering the specific requirements and goals of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Overfitted Models</span>\n",
    "\n",
    "## <span style=\"color:blue\">Random Forest (RF) with 17 features (Model 11)</span>\n",
    "- Train R-squared: 0.9147\n",
    "- Test R-squared: 0.5216\n",
    "- **Comments:** There's a significant difference between training and test R-squared, suggesting potential overfitting on the training set.\n",
    "\n",
    "## <span style=\"color:blue\">Random Forest (RF) with 15 features (Model 12)</span>\n",
    "- Train R-squared: 0.9057\n",
    "- Test R-squared: 0.4710\n",
    "- **Comments:** Similar to Model 11, overfitting is observed.\n",
    "\n",
    "## <span style=\"color:blue\">Tuned RF with 17 features (Models 13 and 14)</span>\n",
    "### <span style=\"color:green\">Model 13</span>\n",
    "- Train R-squared: 0.4589\n",
    "- Test R-squared: 0.4624\n",
    "- **Comments:** There's a small improvement over Model 11, but overfitting is still evident.\n",
    "\n",
    "### <span style=\"color:green\">Model 14</span>\n",
    "- Train R-squared: 0.4807\n",
    "- Test R-squared: 0.4762\n",
    "- **Comments:** Overfitting is observed, and the model's performance on the test set is not significantly better than the untuned RF.\n",
    "\n",
    "## <span style=\"color:blue\">Tuned RF with 15 features (Model 15)</span>\n",
    "- Train R-squared: 0.4762\n",
    "- Test R-squared: 0.4702\n",
    "- **Comments:** Similar to Model 14, overfitting is observed.\n",
    "\n",
    "## <span style=\"color:blue\">RF-optimized with 17 features (Models 16 and 17)</span>\n",
    "### <span style=\"color:purple\">Model 16</span>\n",
    "- Train R-squared: 0.6758\n",
    "- Test R-squared: 0.5118\n",
    "- **Comments:** There's a significant difference between training and test R-squared, suggesting potential overfitting on the training set.\n",
    "\n",
    "### <span style=\"color:purple\">Model 17</span>\n",
    "- Train R-squared: 0.6537\n",
    "- Test R-squared: 0.5148\n",
    "- **Comments:** Similar to Model 16, overfitting is observed.\n",
    "\n",
    "## <span style=\"color:blue\">RF-optimized with 15 features (Model 18)</span>\n",
    "- Train R-squared: 0.6537\n",
    "- Test R-squared: 0.5148\n",
    "- **Comments:** Similar to Models 16 and 17, overfitting is observed.\n",
    "\n",
    "## <span style=\"color:blue\">XGBoost with 17 features (Models 26 and 27)</span>\n",
    "### <span style=\"color:orange\">Model 26</span>\n",
    "- Train R-squared: 0.0573\n",
    "- Test R-squared: 0.0336\n",
    "- **Comments:** There's a significant difference between training and test R-squared, indicating potential overfitting.\n",
    "\n",
    "### <span style=\"color:orange\">Model 27</span>\n",
    "- Train R-squared: 0.0573\n",
    "- Test R-squared: 0.0336\n",
    "- **Comments:** Similar to Model 26.\n",
    "\n",
    "## <span style=\"color:blue\">XGBoost with 15 features (Model 28)</span>\n",
    "- Train R-squared: -0.0854\n",
    "- Test R-squared: -0.0994\n",
    "- **Comments:** There's a significant difference between training and test R-squared, indicating potential overfitting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
